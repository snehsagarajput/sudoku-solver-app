{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "1. Train MNIST Data",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQcGY_PvR8Q0"
      },
      "source": [
        "# 1. **Train MNIST Data**\n",
        "`We will train the first model on the MNIST-dataset. The MNIST-dataset contains 70,000 images (28 x 28 pixels) of handwritten digits. `\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DShadFdTCde"
      },
      "source": [
        "!pip install numpy==1.18.5\n",
        "!pip install matplotlib==3.2.2\n",
        "!pip install tensorflow==2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glHZJdgSR8Q6"
      },
      "source": [
        "# Importing useful libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D, Activation, BatchNormalization, MaxPool2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 30\n",
        "LR=1e-3\n",
        "\n",
        "# Input image dimensions\n",
        "img_rows, img_cols = 28, 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3l5cWsmR8Q-"
      },
      "source": [
        "# Import the data, split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalizing the input and reshaping\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "X_train /= 255\n",
        "X_test = X_test.reshape(X_test.shape[0],28,28,1)\n",
        "X_test = X_test.astype('float32')\n",
        "X_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1KvTTD6R8RD"
      },
      "source": [
        "# Applying transformation to image\n",
        "train_gen = ImageDataGenerator(rotation_range=8,\n",
        "                               width_shift_range=0.08,\n",
        "                               shear_range=0.3,\n",
        "                               height_shift_range=0.08,\n",
        "                               zoom_range=0.08)\n",
        "test_gen = ImageDataGenerator()\n",
        "training_set = train_gen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
        "test_set = train_gen.flow(X_test, y_test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZhC-MK-R8RA"
      },
      "source": [
        "def DigitNet(height=28, width=28, depth=1, classes=10):  # best bs=64 & 128\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n",
        "                     activation='relu', input_shape=(height, width, depth)))\n",
        "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n",
        "                     activation='relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
        "                     activation='relu'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
        "                     activation='relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(classes, activation=\"softmax\"))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN2Dey4NR8RH"
      },
      "source": [
        "## Training the model\n",
        "`We will use an earlystopper, which is a functionality provided by the Keras API. We can specify the earlystopper to monitor the validation loss. Once the validation loss stops improving (decreasing), the earlystopper stops the training session to mitigate the risk of overfitting.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8toijvCuBkRj"
      },
      "source": [
        "model = DigitNet(28, 28, 1, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF7IwRK-EXC4"
      },
      "source": [
        "# Compiling\n",
        "checkpoint = tf.compat.v1.keras.callbacks.ModelCheckpoint(\n",
        "    'BestDigitNet.pb', monitor='val_loss', save_best_only=True, mode='auto')  # Callback for Model with best validation loss\n",
        "earlystopper = tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "adam = Adam(lr=LR)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=adam, metrics=[\"accuracy\"])\n",
        "\n",
        "# Training\n",
        "session = model.fit(training_set,epochs=EPOCHS,\n",
        "                              steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "                              validation_data=test_set,\n",
        "                              validation_steps=X_test.shape[0] // BATCH_SIZE,\n",
        "                              callbacks=[checkpoint,earlystopper])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PCTIezm-X3L"
      },
      "source": [
        "acc = session.history['accuracy']\n",
        "val_acc = session.history['val_accuracy']\n",
        "\n",
        "loss = session.history['loss']\n",
        "val_loss = session.history['val_accuracy']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "# Plot the accuracy\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "# Plot the loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7QM4n_FR8RP"
      },
      "source": [
        "We can see that we get an amazing accuracy of roughly 99% on both the training and the validation set, with the validation set having a little better accuracy. We can also see that earlystopper kicked in and we see no signs of overfitting in the two plots above. Lastly, we have to save the model so we can import it in the next jupyter-notebook, where we will use Transfer learning to train our final model to be used for Digit Classification.\n",
        "\n",
        "\n",
        "***Check For BestDigitNet.pb folder for our saved model.***\n",
        "\n"
      ]
    }
  ]
}