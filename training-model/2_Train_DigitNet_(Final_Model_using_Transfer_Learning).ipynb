{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "2. Train DigitNet (Final Model using Transfer Learning)",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHh4o1WsG71W"
      },
      "source": [
        "#2. **Train DigitNet (Final Model using Transfer Learning)**\n",
        "\n",
        "Here we will train the final model using transfer\n",
        "learning. The idea of transfer learning is to take\n",
        "knowledge gained from solving one problem and apply this knowledge on a similar, but different, problem. We can transfer the knowledge gathered from the MNIST-dataset and apply it to our dataset. In this case, we can take our model trained on the MNIST-dataset, freeze all parameters, change the output layer and then train the model on our the new dataset. By freezing the parameters from the old model we can ensure that only the last classification layer gets trained. A nice additional benefit from this is that it will also reduce the training time significantly because we are only training the variables of our last classification layer and not the entire model.\n",
        "\n",
        "The dataset used is provided by [EmpanS](https://github.com/Empans). It has almost 1282 labeled pictures which we now can use to train our final model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUjry461G71Y"
      },
      "source": [
        "# Install useful dependencies\n",
        "!pip install numpy==1.18.5\n",
        "!pip install matplotlib==3.2.2\n",
        "!pip install tensorflow==2.3.0\n",
        "!pip install opencv-python==4.1.2.30\n",
        "!pip install scikit-learn==0.22.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MEDrTF5G71e"
      },
      "source": [
        "# Importing useful libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import os.path as path\n",
        "try:\n",
        "    import cv2\n",
        "except:\n",
        "    from cv2 import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 9\n",
        "EPOCHS = 50\n",
        "LR=1e-3\n",
        "\n",
        "# Input image dimensions\n",
        "img_rows, img_cols = 28, 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS9PInT4QJ2X"
      },
      "source": [
        "!git clone https://github.com/EmpanS/Project-Sudoku"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amMKp_6YG71o"
      },
      "source": [
        "# First, we need to extract the images from the zip file to a new folder Training images.\n",
        "archive = zipfile.ZipFile('/content/Project-Sudoku/docs/Train Models/Training images - final model.zip')\n",
        "for file in archive.namelist():\n",
        "    archive.extract(file, os.getcwd() + '/dataset')\n",
        "\n",
        "\n",
        "BASE_PATH = os.getcwd() + '/dataset'\n",
        "NUM_EXAMPLES = len(os.listdir(BASE_PATH))\n",
        "DIM = 28\n",
        "\n",
        "# Load training and test data\n",
        "X = np.zeros((NUM_EXAMPLES, DIM, DIM, 1))\n",
        "y = np.zeros((NUM_EXAMPLES,))\n",
        "for idx, image_path in enumerate(os.listdir(BASE_PATH)):\n",
        "    image = cv2.imread(BASE_PATH+\"/\" + image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Normalize and reshape image\n",
        "    image = image / 255.0\n",
        "    image = np.reshape(image, (DIM,DIM,1))\n",
        "    # label 0 represents number 1, ..., label 8 represents number 9\n",
        "    label = int(image_path.split(\"__\")[0]) - 1\n",
        "    X[idx] = image\n",
        "    y[idx] = label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwJ_dE35RxJv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf65tynxG71w"
      },
      "source": [
        "Now we load the model trained on the MNIST-dataset. Then we freeze all parameters and remove the last layer. Then we add a new last layer with 9 neurons since we want to be able to predict the numbers 1-9. The first model is trained on 10 digits (0-9)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIU-w2gNG71x"
      },
      "source": [
        "# Load pre-trained model\n",
        "model = load_model(\"/content/drive/My Drive/Sudoku Solver/BestDigitNet.pb\")\n",
        "\n",
        "# Remove last layer, set layers to un-trainable and add new output layer\n",
        "model.pop()\n",
        "for l in model.layers:\n",
        "    l.trainable = False\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax', name=\"Output\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmm7QKQTZRd-"
      },
      "source": [
        "checkpoint = tf.compat.v1.keras.callbacks.ModelCheckpoint(\n",
        "    'BestDigitNetFinalModel.pb', monitor='val_loss', save_best_only=True, mode='auto')  # Callback for Model with best validation loss\n",
        "earlystopper = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "adam = Adam(lr=LR)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR4wcA1RG711"
      },
      "source": [
        "# Fit the model\n",
        "session = model.fit(X_train, y_train,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[checkpoint,earlystopper])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQWDGg4pZmFp"
      },
      "source": [
        "acc = session.history['accuracy']\n",
        "val_acc = session.history['val_accuracy']\n",
        "\n",
        "loss = session.history['loss']\n",
        "val_loss = session.history['val_accuracy']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "# Plot the accuracy\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "# Plot the loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7ZC_26XG716"
      },
      "source": [
        "We can see above that the earlystopper did not kick in, the validation loss decreased continously throughout the training session, but we have a validation accuracy of 100%. At first sight, this might seem strange, but remember, we are training on computer genererated numbers, so the numbers are very similar and the model will be used to predict computer generated numbers.\n",
        "\n",
        "Now we have our final model, ready to be used in the project to predict numbers! "
      ]
    }
  ]
}